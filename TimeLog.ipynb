{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5ae35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/anhpham/Projects/verse_project\n",
      "libraries imported\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import psycopg2\n",
    "    import psycopg2.extras\n",
    "    import pandas as pd\n",
    "    import opendp\n",
    "    from opendp.transformations import *\n",
    "    from opendp.measurements import *\n",
    "    from opendp.mod import enable_features\n",
    "    enable_features(\"contrib\")\n",
    "    from opendp.mod import binary_search\n",
    "    from opendp.accuracy import discrete_laplacian_scale_to_accuracy\n",
    "    \n",
    "    # Import time module and logging\n",
    "    import time\n",
    "    import logging\n",
    "    #logging.basicConfig(filename='code_execution.log', level=logging.INFO, format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    import os\n",
    "    log_file_path = os.path.join(os.getcwd(), 'code_execution.log')\n",
    "    logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "    print(\"libraries imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing libraries: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3035ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_DB(dbname, user, host, password):\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = psycopg2.connect(dbname=dbname, user=user, host=host, password=password)\n",
    "        try:\n",
    "            # Create a cursor\n",
    "            cur = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "            print(\"Connected to database, cursor defined\")\n",
    "            return conn, cur\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Error creating cursor: {e}\")\n",
    "            # Close the connection in case of an error\n",
    "            conn.close()\n",
    "            return None, None      \n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e2e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_scan(first_file):\n",
    "    #Scanning CSV's into dataframes\n",
    "    df = pd.read_csv(first_file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de5a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_scan(table_name, cursor):\n",
    "    # Execute a query to fetch column names and data from the table\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch column names from the cursor description\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    # Create a DataFrame from the fetched data\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns=column_names)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c8820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, condition):\n",
    "    # Filter the DataFrame based on the given condition\n",
    "    filtered_df = df.query(condition)\n",
    "    # Reset index in the new dataframe\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6eff6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(df, filtered_df, track_column):\n",
    "    # Find the maximum number of rows in the original DataFrame\n",
    "    max_rows = len(df)\n",
    "\n",
    "    # Find the difference in the number of rows\n",
    "    num_rows_diff = len(df) - len(filtered_df)\n",
    "\n",
    "    # Create a DataFrame of zeros with the same columns as low_gpa_df\n",
    "    zeros_df = pd.DataFrame(0, index=range(num_rows_diff), columns=filtered_df.columns)\n",
    "\n",
    "    # Concatenate low_gpa_df and zeros_df vertically\n",
    "    df_padded = pd.concat([filtered_df, zeros_df],ignore_index=True)\n",
    "\n",
    "    # Create a dummy column to track zero-filled rows\n",
    "    # .astype(int)converts boolean TRUE/FALSE to 1-0\n",
    "    # 0 for False(fake data), 1 for True(real data)\n",
    "    zero_filled = ~(df_padded == 0).all(axis=1)\n",
    "    df_padded[track_column] = zero_filled.astype(int)\n",
    "\n",
    "    return df_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d817b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partially pad so that the data contains k*100 % dummy values\n",
    "def DP_pad_data(filtered_df, original_df, k, track_column):\n",
    "#     if not isinstance(filtered_df, pd.DataFrame) or not isinstance(original_df, pd.DataFrame):\n",
    "#         raise ValueError(\"Both filtered_df and original_df should be valid DataFrames.\")\n",
    "    min_size = len(filtered_df)\n",
    "    max_size = len(original_df)\n",
    "    if min_size >= max_size:\n",
    "        raise ValueError(\"filtered_df should have fewer rows than original_df.\")\n",
    "    size_diff = max_size - min_size\n",
    "    \n",
    "    if not 0 < k < 1:\n",
    "        raise ValueError(\"The value of k should be between 0 and 1.\")\n",
    "        \n",
    "    # Calculate the number of rows to pad, but ensure it does not exceed size_diff \n",
    "    # to satisfy k% dummy rows in the total size, this is the cross-formula, this could grow past max_size sometimes\n",
    "    n_rows=int((min_size * k)/(1-k))\n",
    "    \n",
    "    #when total number of rows after pad reaches max_size, meaning n_rows=size_diff #cross maths\n",
    "    max_k = size_diff / (min_size + size_diff)\n",
    "    \n",
    "    if n_rows > size_diff:\n",
    "        print(f\"Sorry, padding up to {k*100:.2f}% dummy in this dataset is not applicable, as it will exceed the size of input data.\\n\"\n",
    "              f\"Your data will be padded to the maximum input size of {max_size}.\")\n",
    "        print(f\"Maximum effective k: {max_k:.3f}, beyond this value the data will be padded to the maximum size.\\n\"\n",
    "              f\"Runtime is now equal to fully private count.\")\n",
    "        \n",
    "    #realistic pad rows might be different because we cannot exceed input (original datasize)\n",
    "    # the min funct ensures padding only til the max allowed\n",
    "    pad_rows = min(n_rows, size_diff)\n",
    "    size_df_padded = min_size + pad_rows\n",
    "    \n",
    "    zeros_df = pd.DataFrame(0, index=range(pad_rows), columns=filtered_df.columns)\n",
    "    df_padded = pd.concat([filtered_df, zeros_df], ignore_index=True)\n",
    "\n",
    "    print(f\"Size padded such that the dataset has {k*100:.2f}% dummy data: {df_padded.shape[0]}\")\n",
    "    print()\n",
    "\n",
    "    \n",
    "    zero_filled = ~(df_padded == 0).all(axis=1)\n",
    "    df_padded[track_column] = zero_filled.astype(int)\n",
    "    \n",
    "    return df_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc23498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(df, df2, merge_type, column_compare):\n",
    "    final_df = pd.merge(left = df, right = df2, how = merge_type, on = column_compare)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0303c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of real data (not padded) in a padded dataframe -> count that hides execution time\n",
    "def count_real(padded_df, track_column):\n",
    "    count_real = (padded_df[track_column] == 1).sum()    \n",
    "    #convert to int (bc how pandas handle boolean)\n",
    "    return count_real.item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be1910f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_real(df, column): \n",
    "    #later worry about which df\n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column in df.columns:\n",
    "        # Calculate the mean for the specified column\n",
    "        mean_value = df[column].mean()\n",
    "        return mean_value\n",
    "    else:\n",
    "        raise ValueError(\"Column '{}' does not exist in the DataFrame.\".format(column_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f8f24",
   "metadata": {},
   "source": [
    "In differential privacy, sensitivity, epsilon, and scale are important concepts used in privacy-preserving mechanisms to control the amount of noise added to query results and ensure privacy protection.\n",
    "\n",
    "Sensitivity:\n",
    "Sensitivity refers to the maximum possible change in the query result when a single individual's data is added or removed from the dataset. It measures how much impact a single individual can have on the query result. The sensitivity value is specific to each query and dataset.\n",
    "\n",
    "Epsilon (ε):\n",
    "Epsilon is a privacy parameter that controls the level of privacy protection provided by a differentially private mechanism. A smaller value of epsilon provides stronger privacy guarantees but may result in noisier query results. On the other hand, a larger value of epsilon provides weaker privacy guarantees but yields less noisy query results.\n",
    "\n",
    "The range of epsilon (ε) in differential privacy is typically between 0 and infinity. However, in practice, epsilon is always a positive value, and it's common to use epsilon values that are greater than or equal to 0 and less than or equal to 1.\n",
    "\n",
    "A smaller epsilon value provides stronger privacy guarantees, meaning that the released data is more protected and less likely to reveal sensitive information about individuals in the dataset. On the other hand, a larger epsilon value provides weaker privacy guarantees, meaning that the released data may be more accurate but less private.\n",
    "\n",
    "The choice of epsilon depends on the specific privacy requirements and the level of privacy protection needed for the application. In general, smaller epsilon values are preferred for scenarios where strong privacy protection is essential, such as medical or financial data. However, larger epsilon values can be used for scenarios where a balance between privacy and data accuracy is acceptable.\n",
    "\n",
    "It's important to note that as epsilon increases, the amount of noise added to the query result decreases, which means the released data becomes less private. Therefore, the selection of an appropriate epsilon value requires a careful trade-off between privacy and utility (accuracy of the released data).\n",
    "\n",
    "Scale:\n",
    "Scale is a parameter used in the Laplace and Gaussian mechanisms to determine the amount of noise added to the query result. For the Laplace mechanism, the scale is directly proportional to sensitivity/epsilon, while for the Gaussian mechanism, the scale is sensitivity/epsilon. A higher scale value means more noise is added, which results in more privacy protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a704ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mechanism_selection():\n",
    "    print(\"Select a mechanism for adding noise:\")\n",
    "    print(\"1. Laplace Noise\")\n",
    "    print(\"2. Gaussian Noise\")\n",
    "    print(\"3. Random Noise\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"Enter your choice (1, 2, or 3): \")\n",
    "        if choice == \"1\":\n",
    "            return \"laplace\"\n",
    "        elif choice == \"2\":\n",
    "            return \"gaussian\"\n",
    "        elif choice == \"3\":\n",
    "            return \"random\"\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 1, 2, or 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccab7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def privacy_parameters():\n",
    "    print(\"In order to add noise using Laplace, Gaussian or Random mechanism, first determine privacy parameters.\")\n",
    "    print(\"Enter the sensitivity (e.g., 1): \")\n",
    "    sensitivity = float(input())\n",
    "    print(\"Enter the epsilon (e.g., 0.1): \")\n",
    "    epsilon = float(input())\n",
    "    # Calculate the scale parameter for Laplace distribution\n",
    "    scale = sensitivity / epsilon\n",
    "    return sensitivity, epsilon, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfdd7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(result, mechanism, scale):\n",
    "    if mechanism == \"laplace\":\n",
    "        return laplace_noise(result,scale)\n",
    "    elif mechanism == \"gaussian\":\n",
    "        return gaussian_noise(result,scale)\n",
    "    elif mechanism == \"random\":\n",
    "        return random_noise(result,scale)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mechanism choice. Supported mechanisms are 'laplace' and 'gaussian'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645563bf",
   "metadata": {},
   "source": [
    "The result is a random draw from the discrete Laplace/Gaussian distribution, centered at the true count of the number of records in the underlying dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5595c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Laplace noise for differential privacy\n",
    "def laplace_noise(result, scale):\n",
    "    #scale here just has to be hard-coded. Because scale is proportional but not equal to sensitivity/epsilon\n",
    "    #there isn't a universal way I've found to calculate scale for laplace without knowing standard deviation.\n",
    "    if isinstance(result, int):\n",
    "        dp_count = make_base_discrete_laplace(scale) \n",
    "    if isinstance(result, float):\n",
    "        dp_count = make_base_laplace(scale) \n",
    "    noisy_result = dp_count(result)\n",
    "    return noisy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41a1e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise(result,scale):\n",
    "    if isinstance(result, int):\n",
    "        dp_count = make_base_discrete_gaussian(scale)\n",
    "    if isinstance(result, float):\n",
    "        dp_count = make_base_gaussian(scale) \n",
    "    noisy_result = dp_count(result)\n",
    "    return noisy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a042ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_noise(result, scale):\n",
    "    # Generate Laplace noise and add it to the result\n",
    "    #scale is hard-coded, explanation above\n",
    "    if isinstance(result, int):\n",
    "        noisy_result = int(result + np.random.laplace(loc=0, scale=scale))\n",
    "    if isinstance(result, float):\n",
    "        noisy_result = float(result + np.random.laplace(loc=0, scale=scale))\n",
    "    return noisy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be08bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_deviation(scale, alpha=None):\n",
    "    if alpha is None:\n",
    "        alpha = float(input(\"Enter the alpha (e.g., 0.05): \"))\n",
    "\n",
    "    max_deviation = discrete_laplacian_scale_to_accuracy(scale=scale, alpha=alpha)\n",
    "    return max_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba023b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log_file(log_file_path):\n",
    "    print(\"Trying to read log file:\", log_file_path)\n",
    "    try:\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, 'r') as log_file:\n",
    "                log_contents = log_file.read()\n",
    "            return log_contents\n",
    "        else:\n",
    "            return f\"Log file '{log_file_path}' not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred while reading the log file: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c61882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "def parse_log_content(log_content):\n",
    "    parsed_data = []\n",
    "\n",
    "    for line in log_content.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            try:\n",
    "                # Split based on \"-\"\n",
    "                timestamp_str, event_duration_str = line.split(\" - \", 1)\n",
    "                # Remove trailing white space\n",
    "                timestamp = timestamp_str.strip()\n",
    "                # Split the event and duration string with a maximum of 1 split\n",
    "                event, *duration_str_parts = event_duration_str.split(\": \", 1)\n",
    "                # Join the duration string parts back together in case there were additional separators in the event description\n",
    "                duration_str = \": \".join(duration_str_parts).strip()\n",
    "\n",
    "                if not duration_str:\n",
    "                    raise ValueError(f\"Invalid log format in line: {line}\")\n",
    "\n",
    "                # Check if the duration includes units (e.g., 'ms' or 'seconds')\n",
    "                if 'ms' in duration_str:\n",
    "                    duration = float(duration_str.split(\" ms\")[0])\n",
    "                elif 'seconds' in duration_str:\n",
    "                    duration = float(duration_str.split(\" seconds\")[0]) * 1000  # Convert seconds to milliseconds\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid duration format in line: {line}\")\n",
    "\n",
    "                # Extract the value of 'k' from the event description\n",
    "                k_str = event.split(\"k=\")[1]\n",
    "                k = float(k_str[:-1])  # Remove '%' and convert to a float\n",
    "\n",
    "                parsed_data.append({\"Timestamp\": timestamp, \"Event\": event, \"k\": k, \"Duration\": duration})\n",
    "            except ValueError as e:\n",
    "                # Handle the case when the line does not match the expected format, here its the separator -------\n",
    "                print(f\"Warning: Skipping line due to invalid format: {line}\")\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87806c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import re\n",
    "\n",
    "def write_log_GS(parsed_data):\n",
    "    try:\n",
    "        # Create a DataFrame from the parsed data\n",
    "        log_df = pd.DataFrame(parsed_data)\n",
    "\n",
    "        # Extract the 'k' value from the 'Event' column and create a new 'k' column\n",
    "        log_df['k'] = log_df['Event'].str.extract(r'<k=(\\d+\\.\\d+)>')\n",
    "\n",
    "        # Remove the <k=0.25> part from the 'Event' column\n",
    "        log_df['Event'] = log_df['Event'].str.replace(r'<k=\\d+\\.\\d+>', '', regex=True)\n",
    "\n",
    "        # Reorder the columns to match the desired order: \"Timestamp\", \"Event\", \"k\", \"Duration\"\n",
    "        log_df = log_df[[\"Timestamp\", \"Event\", \"k\", \"Duration\"]]\n",
    "\n",
    "        # Rename the columns to match the desired column names\n",
    "        log_df = log_df.rename(columns={\"Timestamp\": \"Time\", \"Event\": \"Event\", \"k\": \"k\", \"Duration\": \"Duration (in ms)\"})\n",
    "\n",
    "        # Authenticate with Google Sheets using the credentials JSON file\n",
    "        gc = gspread.service_account(filename='verse-project-timesheet-f4792514dd22.json')\n",
    "\n",
    "        # Open the Google Sheet by its title\n",
    "        sheet = gc.open('Code Execution Time Log')\n",
    "\n",
    "        # Select the worksheet to put the data\n",
    "        worksheet = sheet.worksheet('Sheet1')\n",
    "\n",
    "        # Create a list of column names\n",
    "        column_names = log_df.columns.tolist()\n",
    "\n",
    "        # Clear the existing data in the worksheet and write the column names\n",
    "        worksheet.clear()\n",
    "        worksheet.append_row(column_names)\n",
    "\n",
    "        # Convert the DataFrame to a list of lists (values) for writing to the Google Sheet\n",
    "        values = log_df.values.tolist()\n",
    "\n",
    "        # Write the values to the Google Sheet, starting from cell A2 (after the column names)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            worksheet.update('A2', values, value_input_option='USER_ENTERED')\n",
    "\n",
    "        print(\"Log data has been written to the Google Sheet.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing data to Google Sheet: {e}\")\n",
    "        print(\"Error details:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f3059d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_execution_time(event_data):\n",
    "    for event, k, time_ms in event_data:\n",
    "        logging.info(f'{event} <k={k}>: {time_ms:.3f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f36fbe42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database, cursor defined\n",
      "\n",
      "1. SELECT COUNT(*) FROM student WHERE gpa < 3;\n",
      "Real result is: 40\n",
      "\n",
      "Size padded such that the dataset has 25.00% dummy data: 53\n",
      "\n",
      "    sid                 sname sex  age  year  gpa  dummy\n",
      "0     4     Sulfate, Barry M.   m   19     2  2.8      1\n",
      "1     7    Sather, Roberto B.   m   22     4  2.2      1\n",
      "2     9       Smith, Joyce A.   f   21     4  2.0      1\n",
      "3    13         Kellerman, S.   f   21     3  2.9      1\n",
      "4    17     Thorton, James Q.   m   28     4  2.7      1\n",
      "5    18                 Gooch   m   26     1  1.4      1\n",
      "6    19             Smith, L.   m   43     4  0.7      1\n",
      "7    21              Surk, K.   m   23     2  2.5      1\n",
      "8    22             Emile, R.   m   18     1  2.0      1\n",
      "9    29          Hamilton, S.   m   21     3  2.8      1\n",
      "10   31           Andrews, R.   m   19     2  2.8      1\n",
      "11   34     Kasten, Norman L.   m   23     2  2.5      1\n",
      "12   38              Auen, B.   m   21     3  2.7      1\n",
      "13   40         Rosemeyer, S.   f   21     3  2.9      1\n",
      "14   42             Trude, T.   m   19     2  2.9      1\n",
      "15   46   Micheal, Zadicki T.   m   22     2  2.7      1\n",
      "16   47     Roger, Blotter N.   m   21     3  1.9      1\n",
      "17   51  Jetplane, Leaving O.   m   30     1  0.0      1\n",
      "18   52          Fy, Clara I.   f   18     2  2.0      1\n",
      "19   55            Glitch, R.   m   19     1  2.8      1\n",
      "20   57     Hiemerschmitz, A.   f   19     1  2.7      1\n",
      "21   59           Ziebart, F.   m   22     4  1.8      1\n",
      "22   60          Calcmity, J.   f   23     3  2.6      1\n",
      "23   61           Kennedy, Ed   m   55     3  2.3      1\n",
      "24   65       Ripper, Jack T.   m   18     1  0.5      1\n",
      "25   66     Altenhaus, Stuart   m   21     4  2.8      1\n",
      "26   69          Heilskov, G.   m   23     4  2.5      1\n",
      "27   72            Barnes, J.   m   45     5  2.3      1\n",
      "28   77      Paull, Thomas H.   m   23     4  2.6      1\n",
      "29   78              Cool, J.   m   18     2  2.6      1\n",
      "30   80        Connors, Jimmy   m   25     1  0.2      1\n",
      "31   81         Smith, Ike Z.   m   33     1  1.1      1\n",
      "32   82         News, Nightly   m   15     1  1.9      1\n",
      "33   84             Smith, R.   m   19     3  2.7      1\n",
      "34   92              Kirk, J.   m   34     5  2.2      1\n",
      "35   95          Grzlbltz, Q.   m   43     5  2.5      1\n",
      "36   98            Taylor, R.   m   20     3  2.8      1\n",
      "37   99             Jones, J.   f   21     1  1.1      1\n",
      "38  100            Gringo, C.   m   25     5  2.9      1\n",
      "39  104     Baskett, Wayse T.   m   23     3  2.1      1\n",
      "40    0                     0   0    0     0  0.0      0\n",
      "41    0                     0   0    0     0  0.0      0\n",
      "42    0                     0   0    0     0  0.0      0\n",
      "43    0                     0   0    0     0  0.0      0\n",
      "44    0                     0   0    0     0  0.0      0\n",
      "45    0                     0   0    0     0  0.0      0\n",
      "46    0                     0   0    0     0  0.0      0\n",
      "47    0                     0   0    0     0  0.0      0\n",
      "48    0                     0   0    0     0  0.0      0\n",
      "49    0                     0   0    0     0  0.0      0\n",
      "50    0                     0   0    0     0  0.0      0\n",
      "51    0                     0   0    0     0  0.0      0\n",
      "52    0                     0   0    0     0  0.0      0\n",
      "\n",
      "In order to add noise using Laplace, Gaussian or Random mechanism, first determine privacy parameters.\n",
      "Enter the sensitivity (e.g., 1): \n",
      "1\n",
      "Enter the epsilon (e.g., 0.1): \n",
      "0.5\n",
      "\n",
      "Select a mechanism for adding noise:\n",
      "1. Laplace Noise\n",
      "2. Gaussian Noise\n",
      "3. Random Noise\n",
      "Enter your choice (1, 2, or 3): 2\n",
      "Selected mechanism: Gaussian Noise\n",
      "DP Count with gaussian is: 41\n",
      "Max deviation for count at a 95% confidence level: 6.430\n",
      "When the gaussian distribution’s scale is 2.0,\n",
      "the DP estimate differs from the exact estimate by no more than 6.430 at a 95% confidence level.\n",
      "\n",
      "TIME to 3 decimal \n",
      "Trying to read log file: /Users/anhpham/Projects/verse_project/code_execution.log\n",
      "log content\n",
      "2023-07-28 16:44:36 - Scan <k=0.15>: 3.810 ms\n",
      "2023-07-28 16:44:36 - Filter <k=0.15>: 1.574 ms\n",
      "2023-07-28 16:44:36 - Pad <k=0.15>: 1.147 ms\n",
      "2023-07-28 16:44:36 - Fully Public <k=0.15>: 0.002 ms\n",
      "2023-07-28 16:44:36 - Fully Private <k=0.15>: 0.166 ms\n",
      "2023-07-28 16:44:36 - DP Private <k=0.15>: 0.138 ms\n",
      "2023-07-28 16:44:36 - --------------------------------------\n",
      "2023-07-28 16:45:06 - Scan <k=0.15>: 2.694 ms\n",
      "2023-07-28 16:45:06 - Filter <k=0.15>: 1.563 ms\n",
      "2023-07-28 16:45:06 - Pad <k=0.15>: 1.202 ms\n",
      "2023-07-28 16:45:06 - Fully Public <k=0.15>: 0.001 ms\n",
      "2023-07-28 16:45:06 - Fully Private <k=0.15>: 0.170 ms\n",
      "2023-07-28 16:45:06 - DP Private <k=0.15>: 0.116 ms\n",
      "2023-07-28 16:45:06 - --------------------------------------\n",
      "2023-07-28 16:45:41 - Scan <k=0.25>: 2.798 ms\n",
      "2023-07-28 16:45:41 - Filter <k=0.25>: 1.704 ms\n",
      "2023-07-28 16:45:41 - Pad <k=0.25>: 1.265 ms\n",
      "2023-07-28 16:45:41 - Fully Public <k=0.25>: 0.001 ms\n",
      "2023-07-28 16:45:41 - Fully Private <k=0.25>: 0.160 ms\n",
      "2023-07-28 16:45:41 - DP Private <k=0.25>: 0.120 ms\n",
      "2023-07-28 16:45:41 - --------------------------------------\n",
      "\n",
      "Warning: Skipping line due to invalid format: 2023-07-28 16:44:36 - --------------------------------------\n",
      "Warning: Skipping line due to invalid format: 2023-07-28 16:45:06 - --------------------------------------\n",
      "Warning: Skipping line due to invalid format: 2023-07-28 16:45:41 - --------------------------------------\n",
      "Log data has been written to the Google Sheet.\n",
      "Log Data:\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:44:36', 'Event': 'Scan <k=0.15>', 'k': 0.15, 'Duration': 3.81}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:44:36', 'Event': 'Filter <k=0.15>', 'k': 0.15, 'Duration': 1.574}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:44:36', 'Event': 'Pad <k=0.15>', 'k': 0.15, 'Duration': 1.147}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:44:36', 'Event': 'Fully Public <k=0.15>', 'k': 0.15, 'Duration': 0.002}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:44:36', 'Event': 'Fully Private <k=0.15>', 'k': 0.15, 'Duration': 0.166}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:44:36', 'Event': 'DP Private <k=0.15>', 'k': 0.15, 'Duration': 0.138}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:06', 'Event': 'Scan <k=0.15>', 'k': 0.15, 'Duration': 2.694}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:06', 'Event': 'Filter <k=0.15>', 'k': 0.15, 'Duration': 1.563}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:06', 'Event': 'Pad <k=0.15>', 'k': 0.15, 'Duration': 1.202}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:06', 'Event': 'Fully Public <k=0.15>', 'k': 0.15, 'Duration': 0.001}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:06', 'Event': 'Fully Private <k=0.15>', 'k': 0.15, 'Duration': 0.17}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:06', 'Event': 'DP Private <k=0.15>', 'k': 0.15, 'Duration': 0.116}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:41', 'Event': 'Scan <k=0.25>', 'k': 0.25, 'Duration': 2.798}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:41', 'Event': 'Filter <k=0.25>', 'k': 0.25, 'Duration': 1.704}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:41', 'Event': 'Pad <k=0.25>', 'k': 0.25, 'Duration': 1.265}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:41', 'Event': 'Fully Public <k=0.25>', 'k': 0.25, 'Duration': 0.001}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:41', 'Event': 'Fully Private <k=0.25>', 'k': 0.25, 'Duration': 0.16}\n",
      "PARSED DATA {'Timestamp': '2023-07-28 16:45:41', 'Event': 'DP Private <k=0.25>', 'k': 0.25, 'Duration': 0.12}\n",
      "\n",
      "Connection to database closed. Users can repeat the process or try with another database after this.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Connect to the Postgress database and create cursor specific to that database\n",
    "    conn, cur = postgres_DB('university', 'anhpham', 'localhost', 'freedom')\n",
    "    # Check if the connection and cursor are valid\n",
    "    if conn and cur:  \n",
    "        try:\n",
    "            '''\n",
    "            GPA < 3 QUERY\n",
    "            '''\n",
    "            #\"\"\"\n",
    "            print()\n",
    "            print(\"1. SELECT COUNT(*) FROM student WHERE gpa < 3;\")\n",
    "\n",
    "            # record start time\n",
    "            start_scan = time.time()\n",
    "            \n",
    "            # SCAN: Specify the table name to fetch data -> Original Dataframe\n",
    "            table_name = 'student'\n",
    "            df = postgres_scan(table_name, cur)\n",
    "            \n",
    "            # record end time\n",
    "            end_scan = time.time()\n",
    "            scan_time = (end_scan-start_scan) * 10**3\n",
    "            \n",
    "            # record start time\n",
    "            start_filter = time.time()\n",
    "        \n",
    "            #FILTER daframe\n",
    "            filtered_column = 'gpa'\n",
    "            condition = f\"{filtered_column} <3\"\n",
    "\n",
    "            low_gpa_df = filter_df(df, condition)\n",
    "            \n",
    "            # record end time\n",
    "            end_filter = time.time()\n",
    "            filter_time = (end_filter-start_filter) * 10**3\n",
    "            \n",
    "            # record start time\n",
    "            start_public=time.time()\n",
    "            \n",
    "            #FULLY PUBLIC \n",
    "            gpa_count = low_gpa_df.shape[0]\n",
    "            \n",
    "            # record end time\n",
    "            end_public = time.time()\n",
    "            public_time = (end_public-start_public) * 10**3\n",
    "            \n",
    "            # record start time\n",
    "            start_pad = time.time()\n",
    "            \n",
    "            #PAD data\n",
    "            low_gpa_padded=pad_data(df,low_gpa_df, \"dummy\")\n",
    "            #print(low_gpa_padded)\n",
    "            \n",
    "            # record end time\n",
    "            end_pad = time.time()\n",
    "            pad_time = (end_pad-start_pad) * 10**3\n",
    "            \n",
    "            # record start time\n",
    "            start_priv=time.time()\n",
    "\n",
    "            #FULLY PRIVATE\n",
    "            # Call count_real function -> hide execution time when count            \n",
    "            real_count = count_real(low_gpa_padded, 'dummy') \n",
    "            print(\"Real result is:\", real_count)\n",
    "            print()\n",
    "            \n",
    "            # record end time\n",
    "            end_priv = time.time()\n",
    "            full_priv_time = (end_priv-start_priv) * 10**3\n",
    "            \n",
    "            #DIFFERENTIAL PRIVATE PAD\n",
    "            k=0.25\n",
    "            dp_padded = DP_pad_data(low_gpa_df,df, k, \"dummy\")\n",
    "            print(dp_padded)\n",
    "            print()\n",
    "            \n",
    "            # record start time\n",
    "            start_dp = time.time()\n",
    "            \n",
    "            #get real result from this DP padded data, it should still be the same as real result\n",
    "            dp_padded_count = count_real(dp_padded, 'dummy')\n",
    "            \n",
    "            # record end time\n",
    "            end_dp = time.time()\n",
    "            dp_padded_time = (end_dp-start_dp) * 10**3\n",
    "            \n",
    "            #Add noise to DP Pad\n",
    "            \n",
    "             #Get privacy parameters for adding noise\n",
    "            sensitivity, epsilon, scale = privacy_parameters()\n",
    "            print()\n",
    "            \n",
    "            count_mechanism = mechanism_selection()\n",
    "            print(f\"Selected mechanism: {count_mechanism.capitalize()} Noise\")           \n",
    "\n",
    "            #Add DP noise based on the selected mechanism\n",
    "            dp_result = add_noise(real_count, count_mechanism, scale)\n",
    "            print(\"DP Count with\", count_mechanism, \"is:\", dp_result)\n",
    "            \n",
    "            count_deviation=max_deviation(scale, 0.05)\n",
    "            print(\"Max deviation for count at a 95% confidence level: {:.3f}\".format(count_deviation))\n",
    "            print(f\"When the {count_mechanism} distribution’s scale is {scale},\\nthe DP estimate differs from the exact estimate by no more than {count_deviation:.3f} at a 95% confidence level.\")\n",
    "            print() \n",
    "\n",
    "            # print the difference between start and end time in milli. secs\n",
    "            print(\"TIME to 3 decimal \")        \n",
    "            events = [\n",
    "                ('Scan', k, scan_time),\n",
    "                ('Filter', k, filter_time),\n",
    "                ('Pad', k, pad_time),\n",
    "                ('Fully Public', k, public_time),\n",
    "                ('Fully Private', k, full_priv_time),\n",
    "                ('DP Private', k, dp_padded_time)\n",
    "            ]\n",
    "\n",
    "            # logging the time duration of each event\n",
    "            log_execution_time(events)\n",
    "            logging.info('-' * 38)\n",
    "            \n",
    "            # Open and read log\n",
    "            #log_file_path = 'code_execution.log'\n",
    "            log_content = read_log_file(log_file_path)\n",
    "            print(f\"log content\\n{log_content}\")\n",
    "            # Parse the log content to a structured format\n",
    "            parsed_data = parse_log_content(log_content)\n",
    "\n",
    "            # Call the write_log_GS function with the parsed data\n",
    "            write_log_GS(parsed_data)\n",
    "\n",
    "            # Print the log data for verification\n",
    "            print(\"Log Data:\")\n",
    "            for entry in parsed_data:\n",
    "                print(\"PARSED DATA\", entry)\n",
    "\n",
    "            #\"\"\"\n",
    "            \n",
    "            '''\n",
    "            MEAN GPA FROM STUDENT\n",
    "            '''\n",
    "            \n",
    "            \"\"\"\n",
    "            print()\n",
    "            print(\"2. SELECT AVG(gpa) AS average_gpa FROM student;\")\n",
    "            \n",
    "            mean_gpa = mean_real(df,\"gpa\")\n",
    "            print(\"Real result is:\", mean_gpa)\n",
    "            print()\n",
    "            \n",
    "            sensitivity, epsilon, scale = privacy_parameters()\n",
    "            print()\n",
    "            \n",
    "            mean_mechanism = mechanism_selection()\n",
    "            print(f\"Selected mechanism: {mean_mechanism.capitalize()} Noise\") \n",
    "            \n",
    "            dp_mean=add_noise(mean_gpa, mean_mechanism, scale)\n",
    "            print(\"DP Mean GPA with {} is: {:.3f}\".format(mean_mechanism, dp_mean))\n",
    "            \n",
    "            mean_deviation=max_deviation(scale, 0.05)\n",
    "            #print(\"Max deviation for mean at a 95% confidence level: {:.3f}\".format(mean_deviation))\n",
    "            print(f\"Max deviation for mean at a 95% confidence level: {mean_deviation:.3f}\")\n",
    "            print(\"When the {} distribution’s scale is {},\\nthe DP estimate differs from the exact estimate by no more than {:.3f} at a 95% confidence level.\".format(mean_mechanism, scale, mean_deviation))\n",
    "            print()\n",
    "            \n",
    "            print(\"TIME\")\n",
    "            #NEED TO limit THE MEAN VALUE BETWEEN 0-4, working on that right now\n",
    "             \n",
    "            \"\"\"\n",
    "            \n",
    "            '''\n",
    "            NUMPHDS > 65\n",
    "            '''\n",
    "            \n",
    "            \"\"\"\n",
    "            print()\n",
    "            print(\"3. SELECT count(*) FROM course, dept WHERE course.dname = dept.dname AND numphds > 65\")\n",
    "            \n",
    "            table_name = 'course'\n",
    "            df1 = postgres_scan(table_name, cur)\n",
    "            \n",
    "            table_name = 'dept'\n",
    "            df2 = postgres_scan(table_name, cur)\n",
    "            \n",
    "            #Filter dataframe\n",
    "            filtered_column = 'numphds'\n",
    "            condition = f\"{filtered_column} >65\"\n",
    "\n",
    "            df_filtered = filter_df(df2, condition)\n",
    "            \n",
    "            df_joined = join(df1, df_filtered, 'inner', 'dname')\n",
    "            \n",
    "            #Pad data\n",
    "            large_PHD_padded=pad_data(df2, df_joined, \"dummy\")\n",
    "            \n",
    "            # Call count_real function -> hide execution time when count\n",
    "            real_result = count_real(large_PHD_padded, 'dummy') \n",
    "            print(\"Real result is:\", real_result)\n",
    "            print()\n",
    "            \n",
    "            #Get privacy parameters\n",
    "            sensitivity, epsilon, scale = privacy_parameters()\n",
    "\n",
    "            join_mechanism = mechanism_selection()\n",
    "            print(f\"Selected mechanism: {join_mechanism.capitalize()} Noise\")\n",
    "            print()\n",
    "\n",
    "            # Add DP noise based on the selected mechanism\n",
    "            dp_result = add_noise(real_result, join_mechanism, scale)\n",
    "            print(\"DP result with\", join_mechanism, \"is:\", dp_result)\n",
    "            \n",
    "            join_deviation = max_deviation(scale, 0.05)\n",
    "            print(\"Max deviation for count join at a 95% confidence level: {:.3f}\".format(join_deviation))\n",
    "            print(\"When the {} distribution’s scale is {},\\nthe DP estimate differs from the exact estimate by no more than {:.3f} at a 95% confidence level.\".format(join_mechanism, scale, join_deviation))\n",
    "            \"\"\"\n",
    "             \n",
    "        finally:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print()\n",
    "            print(\"Connection to database closed. Users can repeat the process or try with another database after this.\")\n",
    "            \n",
    "    #start writing csv code here without interfering with postgress code   \n",
    "    \n",
    "    #scan data\n",
    "    '''\n",
    "    # record start time\n",
    "    start_scan = time.time()\n",
    "    \n",
    "    df = csv_scan(\"1KB.csv\")\n",
    "    print(df)\n",
    "    \n",
    "    # record end time\n",
    "    end_scan = time.time()\n",
    "    scan_time = (end_scan-start_scan) * 10**3\n",
    "    \n",
    "    # record start time\n",
    "    start_filter = time.time()\n",
    "        \n",
    "    #filter\n",
    "    filtered_column = 'Age'\n",
    "    condition = f\"{filtered_column} < 26\"\n",
    "\n",
    "    young_df = filter_df(df, condition)\n",
    "    print(young_df)\n",
    "    \n",
    "    # record end time\n",
    "    end_filter = time.time()\n",
    "    filter_time = (end_filter-start_filter) * 10**3\n",
    "    \n",
    "    # record start time\n",
    "    start_public=time.time()\n",
    "    \n",
    "    #count\n",
    "    #FULLY PUBLIC \n",
    "    young_count = young_df.shape[0]\n",
    "\n",
    "    # record end time\n",
    "    end_public = time.time()\n",
    "    public_time = (end_public-start_public) * 10**3\n",
    "\n",
    "    # record start time\n",
    "    start_pad = time.time()\n",
    "\n",
    "    #PAD data\n",
    "    young_padded=pad_data(df,young_df, \"dummy\")\n",
    "    #print(low_gpa_padded)\n",
    "\n",
    "    # record end time\n",
    "    end_pad = time.time()\n",
    "    pad_time = (end_pad-start_pad) * 10**3\n",
    "    \n",
    "    # record start time\n",
    "    start_priv=time.time()\n",
    "\n",
    "    #FULLY PRIVATE\n",
    "    # Call count_real function -> hide execution time when count            \n",
    "    real_count = count_real(young_padded, 'dummy') \n",
    "    print(\"Real result is:\", real_count)\n",
    "    print()\n",
    "\n",
    "    # record end time\n",
    "    end_priv = time.time()\n",
    "    full_priv_time = (end_priv-start_priv) * 10**3\n",
    "    \n",
    "    '''\n",
    "            \n",
    "    \n",
    "       \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75363bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#write some small program to increase file size\n",
    "#to get data points for the x-bar\n",
    "\n",
    "\n",
    "#scan, filter, join, count\n",
    "#extract \n",
    "#run individually\n",
    "#just scan, just joint\n",
    "#if 25% 50% percent dummy then how the performance change\n",
    "#setting 25% dummy\n",
    "\n",
    "\n",
    "#mechanism - getscale\n",
    "#check DP pad\n",
    "#about logging, check if my log is checking each stage? is that what we re trying to do\n",
    "#try putting log in google sheet\n",
    "#visualization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
